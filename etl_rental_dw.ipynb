{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "511cb546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd44bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "user = 'root'\n",
    "password = 'raghad2531'\n",
    "host = 'localhost'\n",
    "port = 3306\n",
    "source_db = 'sales_transactions_dw'\n",
    "target_db = 'rental_star'\n",
    "\n",
    "# Create SQLAlchemy engines for source and target databases\n",
    "source_engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{source_db}')\n",
    "target_engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}:{port}/{target_db}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06822937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_table(query, target_table, key_columns=None, foreign_key_checks=None, drop_na=True, if_exists='append'):\n",
    "    \"\"\"\n",
    "    Extract data from source DB, transform by removing duplicates, optionally dropping NA in key columns,\n",
    "    filtering invalid foreign keys, and avoiding inserting duplicates.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): SQL query to extract data from source\n",
    "    - target_table (str): target table name in destination DB\n",
    "    - key_columns (list[str]): columns to check for dropping NA and deduplication\n",
    "    - foreign_key_checks (dict): { 'fk_column': 'dimension_table' } for filtering invalid foreign keys\n",
    "    - drop_na (bool): whether to drop rows with NA in key_columns\n",
    "    - if_exists (str): pandas to_sql if_exists option (default 'append')\n",
    "    \"\"\"\n",
    "    print(f\"Extracting data for '{target_table}'...\")\n",
    "    df = pd.read_sql(query, source_engine)\n",
    "\n",
    "    print(\"Removing duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    if drop_na and key_columns:\n",
    "        print(f\" Dropping rows with NA in key columns: {key_columns} ...\")\n",
    "        df = df.dropna(subset=key_columns)\n",
    "\n",
    "    # Filter rows with invalid foreign keys if specified\n",
    "    if foreign_key_checks:\n",
    "        for fk_col, dim_table in foreign_key_checks.items():\n",
    "            print(f\" Checking foreign key '{fk_col}' against dimension '{dim_table}'...\")\n",
    "            valid_keys = pd.read_sql(f\"SELECT DISTINCT {fk_col} FROM {dim_table}\", target_engine)\n",
    "            df = df[df[fk_col].isin(valid_keys[fk_col])]\n",
    "    \n",
    "    # Avoid inserting duplicates by excluding keys already in target table\n",
    "    if key_columns:\n",
    "        existing_keys_query = f\"SELECT DISTINCT {', '.join(key_columns)} FROM {target_table}\"\n",
    "        existing_keys_df = pd.read_sql(existing_keys_query, target_engine)\n",
    "        df = df.merge(existing_keys_df, on=key_columns, how='left', indicator=True)\n",
    "        df = df[df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"No new clean data to load into '{target_table}'. Skipping load.\\n\")\n",
    "        return df\n",
    "\n",
    "    print(f\"Loading {len(df)} records into '{target_table}'...\")\n",
    "    df.to_sql(target_table, target_engine, if_exists=if_exists, index=False)\n",
    "    print(f\" Loaded table '{target_table}' successfully.\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b84118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_dim(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Generate a date dimension DataFrame from start_date to end_date.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns: date_id (YYYYMMDD int), full_date, day, month, year\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        dates.append({\n",
    "            'date_id': int(current.strftime('%Y%m%d')),\n",
    "            'full_date': current.date(),\n",
    "            'day': current.day,\n",
    "            'month': current.month,\n",
    "            'year': current.year\n",
    "        })\n",
    "        current += timedelta(days=1)\n",
    "    return pd.DataFrame(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7a1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dim_date():\n",
    "    \"\"\"\n",
    "    Extract min and max payment dates from source and generate/load date dimension.\n",
    "    \"\"\"\n",
    "    query_dates = \"SELECT MIN(payment_date) AS min_date, MAX(payment_date) AS max_date FROM payment\"\n",
    "    date_range = pd.read_sql(query_dates, source_engine)\n",
    "\n",
    "    start_date = pd.to_datetime(date_range['min_date'][0]).replace(day=1)\n",
    "    end_date = pd.to_datetime(date_range['max_date'][0]) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    print(\"Generating and loading date dimension...\")\n",
    "    df_dates = generate_date_dim(start_date, end_date)\n",
    "    df_dates.to_sql(\"dim_date\", target_engine, if_exists='append', index=False)\n",
    "    print(\"Date dimension loaded successfully.\\n\")\n",
    "    return df_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ff584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fact_monthly_payment():\n",
    "    \"\"\"\n",
    "    ETL process for fact_monthly_payment table:\n",
    "    - Extract payment data\n",
    "    - Join with dim_date, dim_staff, dim_rent to filter valid records\n",
    "    - Aggregate monthly payment amount by staff and rental\n",
    "    - Load aggregated results into fact_monthly_payment table\n",
    "    \"\"\"\n",
    "    print(\"Extracting payment data and related dimensions...\")\n",
    "    dim_staff = pd.read_sql('SELECT staff_id FROM dim_staff', target_engine)\n",
    "    dim_rent = pd.read_sql('SELECT rent_id, rental_date FROM dim_rent', target_engine)\n",
    "    dim_date = pd.read_sql('SELECT date_id, full_date FROM dim_date', target_engine)\n",
    "\n",
    "    dim_date['full_date'] = pd.to_datetime(dim_date['full_date']).dt.normalize()\n",
    "\n",
    "    payment = pd.read_sql('SELECT payment_id, staff_id, rental_id, amount, payment_date FROM payment', source_engine)\n",
    "    payment['payment_date_only'] = pd.to_datetime(payment['payment_date']).dt.normalize()\n",
    "\n",
    "    payment = payment.merge(dim_date, left_on='payment_date_only', right_on='full_date', how='left')\n",
    "\n",
    "    # Filter to keep only valid staff and rental IDs\n",
    "    payment = payment[payment['staff_id'].isin(dim_staff['staff_id'])]\n",
    "    payment = payment[payment['rental_id'].isin(dim_rent['rent_id'])]\n",
    "\n",
    "    # Extract year and month for aggregation\n",
    "    payment['year'] = payment['full_date'].dt.year\n",
    "    payment['month'] = payment['full_date'].dt.month\n",
    "\n",
    "    # Aggregate monthly amount by staff_id and rental_id\n",
    "    fact_agg = payment.groupby(['staff_id', 'rental_id', 'year', 'month']).agg({'amount': 'sum'}).reset_index()\n",
    "\n",
    "    # Create a date representing the first day of the month for joining with date dimension\n",
    "    fact_agg['first_day_of_month'] = pd.to_datetime(fact_agg[['year', 'month']].assign(day=1))\n",
    "\n",
    "    fact_agg = fact_agg.merge(dim_date, left_on='first_day_of_month', right_on='full_date', how='left')\n",
    "\n",
    "    fact_final = fact_agg[['date_id', 'staff_id', 'rental_id', 'amount']].copy()\n",
    "    fact_final.rename(columns={'rental_id': 'rent_id'}, inplace=True)\n",
    "\n",
    "    # Create a payment_id sequence\n",
    "    fact_final['payment_id'] = range(1, len(fact_final) + 1)\n",
    "\n",
    "    # Reorder columns for final fact table\n",
    "    fact_final = fact_final[['payment_id', 'date_id', 'staff_id', 'rent_id', 'amount']]\n",
    "\n",
    "    print(f\"Loading aggregated monthly payments into 'fact_monthly_payment'...\")\n",
    "    fact_final.to_sql(\"fact_monthly_payment\", target_engine, if_exists='append', index=False)\n",
    "    print(\"Loaded fact_monthly_payment successfully.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "944f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fact_daily_inventory():\n",
    "    \"\"\"\n",
    "    ETL process for fact_daily_inventory table:\n",
    "    - Extract inventory and dimension tables\n",
    "    - Join and filter by existing dimension keys\n",
    "    - Aggregate inventory count by film_id, store_id and date\n",
    "    - Load aggregated results into fact_daily_inventory table\n",
    "    \"\"\"\n",
    "    print(\" Extracting inventory and dimension tables...\")\n",
    "    dim_film = pd.read_sql('SELECT film_id FROM dim_film', target_engine)\n",
    "    dim_store = pd.read_sql('SELECT store_id FROM dim_store', target_engine)\n",
    "    dim_date = pd.read_sql('SELECT date_id, full_date FROM dim_date', target_engine)\n",
    "    dim_date['full_date'] = pd.to_datetime(dim_date['full_date'])\n",
    "\n",
    "    inventory = pd.read_sql('SELECT inventory_id, film_id, store_id, last_update FROM inventory', source_engine)\n",
    "    inventory['last_update_date'] = pd.to_datetime(inventory['last_update']).dt.normalize()\n",
    "\n",
    "    # Join with date dimension on last_update_date\n",
    "    inventory = inventory.merge(dim_date, left_on='last_update_date', right_on='full_date', how='left')\n",
    "\n",
    "    # Filter to keep only inventory with valid film_id and store_id\n",
    "    inventory = inventory[inventory['film_id'].isin(dim_film['film_id'])]\n",
    "    inventory = inventory[inventory['store_id'].isin(dim_store['store_id'])]\n",
    "\n",
    "    # Aggregate inventory quantity by film, store, and date\n",
    "    fact_agg = inventory.groupby(['film_id', 'store_id', 'date_id']).agg(\n",
    "        inventory_qty=('inventory_id', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    fact_agg['inventory_id'] = range(1, len(fact_agg) + 1)\n",
    "\n",
    "    fact_final = fact_agg[['inventory_id', 'date_id', 'film_id', 'store_id', 'inventory_qty']]\n",
    "\n",
    "    print(f\"Loading daily inventory facts into 'fact_daily_inventory'...\")\n",
    "    fact_final.to_sql('fact_daily_inventory', target_engine, if_exists='append', index=False)\n",
    "    print(\"Loaded fact_daily_inventory successfully.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004c9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl():\n",
    "    \"\"\"\n",
    "    Run ETL on dimension and fact tables, cleaning data before loading.\n",
    "    Make sure to:\n",
    "    1) Run ETL with current clean data.\n",
    "    2) Add some dirty (not cleaned) data manually.\n",
    "    3) Run ETL again to check if it correctly handles duplicates, missing values, and invalid foreign keys.\n",
    "    \"\"\"\n",
    "    # Load dimension tables with checks for keys and foreign keys where applicable\n",
    "    etl_table(\"\"\"\n",
    "        SELECT staff_id, first_name, last_name, email, store_id, active, username, last_update\n",
    "        FROM staff\n",
    "    \"\"\", \"dim_staff\", key_columns=[\"staff_id\"], drop_na=True)\n",
    "\n",
    "    etl_table(\"\"\"\n",
    "        SELECT store_id, manager_staff_id, address_id, last_update\n",
    "        FROM store\n",
    "    \"\"\", \"dim_store\", key_columns=[\"store_id\"], drop_na=True)\n",
    "\n",
    "    etl_table(\"\"\"\n",
    "        SELECT rental_id AS rent_id, rental_date, return_date, inventory_id, customer_id, staff_id, last_update\n",
    "        FROM rental\n",
    "    \"\"\", \"dim_rent\", \n",
    "        key_columns=[\"rent_id\", \"rental_date\"], \n",
    "        foreign_key_checks={\"staff_id\": \"dim_staff\"}, \n",
    "        drop_na=True)\n",
    "\n",
    "    etl_table(\"\"\"\n",
    "        SELECT film_id, title, description, release_year, rental_duration, rental_rate\n",
    "        FROM film\n",
    "    \"\"\", \"dim_film\", key_columns=[\"film_id\"], drop_na=True)\n",
    "\n",
    "    # Load date dimension table\n",
    "    load_dim_date()\n",
    "\n",
    "    # Load fact tables\n",
    "    load_fact_monthly_payment()\n",
    "    load_fact_daily_inventory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4c7d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for 'dim_staff'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['staff_id'] ...\n",
      "Loading 2 records into 'dim_staff'...\n",
      " Loaded table 'dim_staff' successfully.\n",
      "\n",
      "Extracting data for 'dim_store'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['store_id'] ...\n",
      "No new clean data to load into 'dim_store'. Skipping load.\n",
      "\n",
      "Extracting data for 'dim_rent'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['rent_id', 'rental_date'] ...\n",
      "ðŸ” Checking foreign key 'staff_id' against dimension 'dim_staff'...\n",
      "Loading 16044 records into 'dim_rent'...\n",
      " Loaded table 'dim_rent' successfully.\n",
      "\n",
      "Extracting data for 'dim_film'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['film_id'] ...\n",
      "No new clean data to load into 'dim_film'. Skipping load.\n",
      "\n",
      "Generating and loading date dimension...\n",
      "Date dimension loaded successfully.\n",
      "\n",
      "Extracting payment data and related dimensions...\n",
      "Loading aggregated monthly payments into 'fact_monthly_payment'...\n",
      "Loaded fact_monthly_payment successfully.\n",
      "\n",
      " Extracting inventory and dimension tables...\n",
      "Loading daily inventory facts into 'fact_daily_inventory'...\n",
      "Loaded fact_daily_inventory successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d0cb941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before ETL: 2\n"
     ]
    }
   ],
   "source": [
    "def count_rows(table_name, engine):\n",
    "    query = f\"SELECT COUNT(*) as cnt FROM {table_name}\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    return df['cnt'][0]\n",
    "\n",
    "before_count = count_rows('staff', source_engine)\n",
    "print(f\"Rows before ETL: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "204c3385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared staff_params to insert: [{'staff_id': 1, 'first_name': 'Mike', 'last_name': 'Hillyer', 'address_id': 1, 'picture': None, 'email': 'mike.hillyer@sakilastaff.com', 'store_id': 1, 'active': 1, 'username': 'mike', 'last_update': datetime.datetime(2025, 5, 24, 23, 22, 18, 229568)}, {'staff_id': 8, 'first_name': 'John', 'last_name': '', 'address_id': 1, 'picture': None, 'email': 'john.doe@example.com', 'store_id': 1, 'active': 1, 'username': 'johndoe', 'last_update': datetime.datetime(2025, 5, 24, 23, 22, 18, 229568)}, {'staff_id': 3, 'first_name': 'Fake', 'last_name': 'Staff', 'address_id': 1, 'picture': None, 'email': 'fake.staff@example.com', 'store_id': 1, 'active': 1, 'username': 'fakestaff', 'last_update': datetime.datetime(2025, 5, 24, 23, 22, 18, 229568)}]\n",
      "Rows affected in staff: 4\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sqlalchemy import text\n",
    "\n",
    "dirty_staff_data = [\n",
    "    (1, 'Mike', 'Hillyer', 1, None, 'mike.hillyer@sakilastaff.com', 1, 1, 'mike', datetime.now()),\n",
    "    (8, 'John', None, 1, None, 'john.doe@example.com', 1, 1, 'johndoe', datetime.now()),\n",
    "    (3, 'Fake', 'Staff', 1, None, 'fake.staff@example.com', 1, 1, 'fakestaff', datetime.now()),\n",
    "]\n",
    "\n",
    "def insert_dirty_data():\n",
    "    with source_engine.begin() as connection:\n",
    "        insert_staff_query = \"\"\"\n",
    "            INSERT INTO staff (\n",
    "                staff_id, first_name, last_name, address_id, picture,\n",
    "                email, store_id, active, username, last_update\n",
    "            )\n",
    "            VALUES (\n",
    "                :staff_id, :first_name, :last_name, :address_id, :picture,\n",
    "                :email, :store_id, :active, :username, :last_update\n",
    "            )\n",
    "            ON DUPLICATE KEY UPDATE last_update=VALUES(last_update)\n",
    "        \"\"\"\n",
    "        \n",
    "        staff_params = []\n",
    "        for row in dirty_staff_data:\n",
    "            staff_params.append({\n",
    "                'staff_id': row[0],\n",
    "                'first_name': row[1],\n",
    "                'last_name': row[2] if row[2] is not None else '',  # ØªØ¹ÙˆÙŠØ¶ None Ø¨Ù‚ÙŠÙ…Ø© ÙØ§Ø±ØºØ©\n",
    "                'address_id': row[3] if row[3] is not None else 1,\n",
    "                'picture': row[4],\n",
    "                'email': row[5],\n",
    "                'store_id': row[6] if row[6] is not None else 1,\n",
    "                'active': row[7],\n",
    "                'username': row[8],\n",
    "                'last_update': row[9]\n",
    "            })\n",
    "        \n",
    "        print(\"Prepared staff_params to insert:\", staff_params)\n",
    "        \n",
    "        result = connection.execute(text(insert_staff_query), staff_params)\n",
    "        print(f\"Rows affected in staff: {result.rowcount}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    insert_dirty_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf2ac02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before ETL: 4\n"
     ]
    }
   ],
   "source": [
    "before_count = count_rows('staff', source_engine)\n",
    "print(f\"Rows before ETL: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85e46a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data from fact_monthly_payment ...\n",
      "Deleting data from fact_daily_inventory ...\n",
      "Deleting data from dim_date ...\n",
      "Deleting data from dim_staff ...\n",
      "Deleting data from dim_rent ...\n",
      "Data cleared from fact and dimension tables.\n"
     ]
    }
   ],
   "source": [
    "tables_to_clear = [\n",
    "    \"fact_monthly_payment\",\n",
    "    \"fact_daily_inventory\",\n",
    "    \"dim_date\",\n",
    "    \"dim_staff\",\n",
    "    \"dim_rent\",\n",
    "]\n",
    "\n",
    "with target_engine.connect() as conn:\n",
    "    for table in tables_to_clear:\n",
    "        print(f\"Deleting data from {table} ...\")\n",
    "        conn.execute(text(f\"DELETE FROM {table}\"))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Data cleared from fact and dimension tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a6e0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for 'dim_staff'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['staff_id'] ...\n",
      "Loading 4 records into 'dim_staff'...\n",
      " Loaded table 'dim_staff' successfully.\n",
      "\n",
      "Extracting data for 'dim_store'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['store_id'] ...\n",
      "No new clean data to load into 'dim_store'. Skipping load.\n",
      "\n",
      "Extracting data for 'dim_rent'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['rent_id', 'rental_date'] ...\n",
      "ðŸ” Checking foreign key 'staff_id' against dimension 'dim_staff'...\n",
      "Loading 16044 records into 'dim_rent'...\n",
      " Loaded table 'dim_rent' successfully.\n",
      "\n",
      "Extracting data for 'dim_film'...\n",
      "Removing duplicates...\n",
      " Dropping rows with NA in key columns: ['film_id'] ...\n",
      "No new clean data to load into 'dim_film'. Skipping load.\n",
      "\n",
      "Generating and loading date dimension...\n",
      "Date dimension loaded successfully.\n",
      "\n",
      "Extracting payment data and related dimensions...\n",
      "Loading aggregated monthly payments into 'fact_monthly_payment'...\n",
      "Loaded fact_monthly_payment successfully.\n",
      "\n",
      " Extracting inventory and dimension tables...\n",
      "Loading daily inventory facts into 'fact_daily_inventory'...\n",
      "Loaded fact_daily_inventory successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_etl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fc4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
